{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.layers\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import missingno\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./dataset/modcloth_final_data.json', lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [i.replace(\" \",\"_\") for i in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82790 entries, 0 to 82789\n",
      "Data columns (total 18 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   item_id         82790 non-null  int64  \n",
      " 1   waist           2882 non-null   float64\n",
      " 2   size            82790 non-null  int64  \n",
      " 3   quality         82722 non-null  float64\n",
      " 4   cup_size        76535 non-null  object \n",
      " 5   hips            56064 non-null  float64\n",
      " 6   bra_size        76772 non-null  float64\n",
      " 7   category        82790 non-null  object \n",
      " 8   bust            11854 non-null  object \n",
      " 9   height          81683 non-null  object \n",
      " 10  user_name       82790 non-null  object \n",
      " 11  length          82755 non-null  object \n",
      " 12  fit             82790 non-null  object \n",
      " 13  user_id         82790 non-null  int64  \n",
      " 14  shoe_size       27915 non-null  float64\n",
      " 15  shoe_width      18607 non-null  object \n",
      " 16  review_summary  76065 non-null  object \n",
      " 17  review_text     76065 non-null  object \n",
      "dtypes: float64(5), int64(3), object(10)\n",
      "memory usage: 11.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>waist</th>\n",
       "      <th>size</th>\n",
       "      <th>quality</th>\n",
       "      <th>hips</th>\n",
       "      <th>category</th>\n",
       "      <th>bust</th>\n",
       "      <th>height</th>\n",
       "      <th>user_name</th>\n",
       "      <th>length</th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>shoe_size</th>\n",
       "      <th>shoe_width</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123373</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>new</td>\n",
       "      <td>36</td>\n",
       "      <td>5ft 6in</td>\n",
       "      <td>Emily</td>\n",
       "      <td>just right</td>\n",
       "      <td>small</td>\n",
       "      <td>991571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123373</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5ft 2in</td>\n",
       "      <td>sydneybraden2001</td>\n",
       "      <td>just right</td>\n",
       "      <td>small</td>\n",
       "      <td>587883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123373</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5ft 7in</td>\n",
       "      <td>Ugggh</td>\n",
       "      <td>slightly long</td>\n",
       "      <td>small</td>\n",
       "      <td>395665</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alexmeyer626</td>\n",
       "      <td>just right</td>\n",
       "      <td>fit</td>\n",
       "      <td>875643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5ft 2in</td>\n",
       "      <td>dberrones1</td>\n",
       "      <td>slightly long</td>\n",
       "      <td>small</td>\n",
       "      <td>944840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82785</th>\n",
       "      <td>807722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outerwear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5ft 8in</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>just right</td>\n",
       "      <td>fit</td>\n",
       "      <td>727820</td>\n",
       "      <td>8.5</td>\n",
       "      <td>average</td>\n",
       "      <td>Cute jacket!</td>\n",
       "      <td>Cute jacket!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82786</th>\n",
       "      <td>807722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outerwear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5ft 5in</td>\n",
       "      <td>Kelli</td>\n",
       "      <td>slightly long</td>\n",
       "      <td>small</td>\n",
       "      <td>197040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's a beautiful jacket.</td>\n",
       "      <td>It's a beautiful jacket. I love how it's knit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82787</th>\n",
       "      <td>807722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>outerwear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5ft 4in</td>\n",
       "      <td>elacount</td>\n",
       "      <td>just right</td>\n",
       "      <td>fit</td>\n",
       "      <td>102493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love this blazer. It is</td>\n",
       "      <td>I love this blazer. It is a great office piece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82788</th>\n",
       "      <td>807722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>outerwear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5ft 3in</td>\n",
       "      <td>jennaklinner</td>\n",
       "      <td>just right</td>\n",
       "      <td>fit</td>\n",
       "      <td>756491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love this blazer!! I wo</td>\n",
       "      <td>I love this blazer!! I wore it yesterday and g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82789</th>\n",
       "      <td>807722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>outerwear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6ft</td>\n",
       "      <td>maireadsteadman</td>\n",
       "      <td>just right</td>\n",
       "      <td>fit</td>\n",
       "      <td>78305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love this piece. I'm re</td>\n",
       "      <td>I love this piece. I'm really happy with it!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82790 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id  waist  size  quality  hips   category bust   height  \\\n",
       "0       123373   29.0     7      5.0  38.0        new   36  5ft 6in   \n",
       "1       123373   31.0    13      3.0  30.0        new  NaN  5ft 2in   \n",
       "2       123373   30.0     7      2.0   NaN        new  NaN  5ft 7in   \n",
       "3       123373    NaN    21      5.0   NaN        new  NaN      NaN   \n",
       "4       123373    NaN    18      5.0   NaN        new  NaN  5ft 2in   \n",
       "...        ...    ...   ...      ...   ...        ...  ...      ...   \n",
       "82785   807722    NaN     8      4.0   NaN  outerwear  NaN  5ft 8in   \n",
       "82786   807722    NaN    12      5.0   NaN  outerwear  NaN  5ft 5in   \n",
       "82787   807722    NaN    12      5.0  36.0  outerwear  NaN  5ft 4in   \n",
       "82788   807722    NaN    12      4.0   NaN  outerwear  NaN  5ft 3in   \n",
       "82789   807722    NaN     4      4.0  39.0  outerwear  NaN      6ft   \n",
       "\n",
       "              user_name         length    fit  user_id  shoe_size shoe_width  \\\n",
       "0                 Emily     just right  small   991571        NaN        NaN   \n",
       "1      sydneybraden2001     just right  small   587883        NaN        NaN   \n",
       "2                 Ugggh  slightly long  small   395665        9.0        NaN   \n",
       "3          alexmeyer626     just right    fit   875643        NaN        NaN   \n",
       "4            dberrones1  slightly long  small   944840        NaN        NaN   \n",
       "...                 ...            ...    ...      ...        ...        ...   \n",
       "82785          Jennifer     just right    fit   727820        8.5    average   \n",
       "82786             Kelli  slightly long  small   197040        NaN        NaN   \n",
       "82787          elacount     just right    fit   102493        NaN        NaN   \n",
       "82788      jennaklinner     just right    fit   756491        NaN        NaN   \n",
       "82789   maireadsteadman     just right    fit    78305        NaN        NaN   \n",
       "\n",
       "                  review_summary  \\\n",
       "0                            NaN   \n",
       "1                            NaN   \n",
       "2                            NaN   \n",
       "3                            NaN   \n",
       "4                            NaN   \n",
       "...                          ...   \n",
       "82785               Cute jacket!   \n",
       "82786   It's a beautiful jacket.   \n",
       "82787  I love this blazer. It is   \n",
       "82788  I love this blazer!! I wo   \n",
       "82789  I love this piece. I'm re   \n",
       "\n",
       "                                             review_text  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "82785                                       Cute jacket!  \n",
       "82786  It's a beautiful jacket. I love how it's knit ...  \n",
       "82787  I love this blazer. It is a great office piece...  \n",
       "82788  I love this blazer!! I wore it yesterday and g...  \n",
       "82789       I love this piece. I'm really happy with it!  \n",
       "\n",
       "[82790 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['cup_size','bra_size'],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data that can be converted into numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'wide', 'average', 'narrow'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.bust.unique()\n",
    "df.height.unique()\n",
    "df.length.unique()\n",
    "df.shoe_width.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bust(data):\n",
    "    try :\n",
    "        if pd.notnull(data) : ##standard size 37-39\n",
    "            if \"-\" in data:\n",
    "                assert len(data.split(\"-\")) == 2\n",
    "                return np.mean([int(num) for num in data.split(\"-\")])\n",
    "            else :\n",
    "                return int(data)\n",
    "    except Exception as e :\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def normalize_height(data): ##Convert into cms\n",
    "    if pd.notnull(data):\n",
    "        try:\n",
    "            return (int(data[0])*30.48) + (int(data[4:-2])*2.54)\n",
    "        except:\n",
    "            return (int(data[0])*30.48)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bust\"] = df[\"bust\"].apply(lambda x:normalize_bust(x))\n",
    "df[\"height\"] = df[\"height\"].apply(lambda x: normalize_height(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>waist</th>\n",
       "      <th>size</th>\n",
       "      <th>quality</th>\n",
       "      <th>hips</th>\n",
       "      <th>bra_size</th>\n",
       "      <th>bust</th>\n",
       "      <th>height</th>\n",
       "      <th>user_id</th>\n",
       "      <th>shoe_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82790.000000</td>\n",
       "      <td>2882.000000</td>\n",
       "      <td>82790.000000</td>\n",
       "      <td>82722.000000</td>\n",
       "      <td>56064.000000</td>\n",
       "      <td>76772.000000</td>\n",
       "      <td>11854.000000</td>\n",
       "      <td>81683.000000</td>\n",
       "      <td>82790.000000</td>\n",
       "      <td>27915.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>469325.229170</td>\n",
       "      <td>31.319223</td>\n",
       "      <td>12.661602</td>\n",
       "      <td>3.949058</td>\n",
       "      <td>40.358501</td>\n",
       "      <td>35.972125</td>\n",
       "      <td>37.499241</td>\n",
       "      <td>165.471906</td>\n",
       "      <td>498849.564718</td>\n",
       "      <td>8.145818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>213999.803314</td>\n",
       "      <td>5.302849</td>\n",
       "      <td>8.271952</td>\n",
       "      <td>0.992783</td>\n",
       "      <td>5.827166</td>\n",
       "      <td>3.224907</td>\n",
       "      <td>4.635117</td>\n",
       "      <td>7.245308</td>\n",
       "      <td>286356.969459</td>\n",
       "      <td>1.336109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>123373.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>91.440000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>314980.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>160.020000</td>\n",
       "      <td>252897.750000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>454030.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>165.100000</td>\n",
       "      <td>497913.500000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>658440.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>170.180000</td>\n",
       "      <td>744745.250000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>807722.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>241.300000</td>\n",
       "      <td>999972.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             item_id        waist          size       quality          hips  \\\n",
       "count   82790.000000  2882.000000  82790.000000  82722.000000  56064.000000   \n",
       "mean   469325.229170    31.319223     12.661602      3.949058     40.358501   \n",
       "std    213999.803314     5.302849      8.271952      0.992783      5.827166   \n",
       "min    123373.000000    20.000000      0.000000      1.000000     30.000000   \n",
       "25%    314980.000000    28.000000      8.000000      3.000000     36.000000   \n",
       "50%    454030.000000    30.000000     12.000000      4.000000     39.000000   \n",
       "75%    658440.000000    34.000000     15.000000      5.000000     43.000000   \n",
       "max    807722.000000    50.000000     38.000000      5.000000     60.000000   \n",
       "\n",
       "           bra_size          bust        height        user_id     shoe_size  \n",
       "count  76772.000000  11854.000000  81683.000000   82790.000000  27915.000000  \n",
       "mean      35.972125     37.499241    165.471906  498849.564718      8.145818  \n",
       "std        3.224907      4.635117      7.245308  286356.969459      1.336109  \n",
       "min       28.000000     20.000000     91.440000       6.000000      5.000000  \n",
       "25%       34.000000     34.000000    160.020000  252897.750000      7.000000  \n",
       "50%       36.000000     36.000000    165.100000  497913.500000      8.000000  \n",
       "75%       38.000000     40.000000    170.180000  744745.250000      9.000000  \n",
       "max       48.000000     59.000000    241.300000  999972.000000     38.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Outlier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing outlier the values beyond [<(Q1- 1.5IQR), >Q3+1.5IQR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windows 11\\AppData\\Local\\Temp\\ipykernel_16120\\991657846.py:1: FutureWarning: The default value of numeric_only in DataFrame.quantile is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  Q1 = df.quantile(0.25)\n",
      "C:\\Users\\windows 11\\AppData\\Local\\Temp\\ipykernel_16120\\991657846.py:2: FutureWarning: The default value of numeric_only in DataFrame.quantile is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  Q3 = df.quantile(0.75)\n"
     ]
    }
   ],
   "source": [
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "Q1.drop([\"item_id\",\"user_id\"], inplace =True)\n",
    "Q3.drop([\"item_id\",\"user_id\"], inplace = True)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windows 11\\AppData\\Local\\Temp\\ipykernel_16120\\3821504640.py:3: FutureWarning: Automatic reindexing on DataFrame vs Series comparisons is deprecated and will raise ValueError in a future version. Do `left, right = left.align(right, axis=1, copy=False)` before e.g. `left == right`\n",
      "  df = df[~((df< (lower_bound)) |(df > (upper_bound))).any(axis=1)]\n"
     ]
    }
   ],
   "source": [
    "# series_align = pd.Series([11, 12, 13, 14, 15,17,18], index=['waist','size','quality','hips','bra_size','bust','height'])\n",
    "# df, series_align = df.align(series_align,axis=1, copy=False)\n",
    "df = df[~((df< (lower_bound)) |(df > (upper_bound))).any(axis=1)]\n",
    "# df = pd.DataFrame(df)\n",
    "# reference_df = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "# df, reference_df = df.align(reference_df, axis=1, copy=False)\n",
    "# df = df[(df >= lower_bound) & (df <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadling missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>waist</th>\n",
       "      <td>96.371036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bust</th>\n",
       "      <td>85.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoe_width</th>\n",
       "      <td>77.200393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoe_size</th>\n",
       "      <td>65.216110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hips</th>\n",
       "      <td>31.618019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_text</th>\n",
       "      <td>8.005894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_summary</th>\n",
       "      <td>8.005894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cup_size</th>\n",
       "      <td>7.325288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bra_size</th>\n",
       "      <td>6.994106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>1.211058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>0.081392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>0.039293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_name</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                num_missing\n",
       "waist             96.371036\n",
       "bust              85.001403\n",
       "shoe_width        77.200393\n",
       "shoe_size         65.216110\n",
       "hips              31.618019\n",
       "review_text        8.005894\n",
       "review_summary     8.005894\n",
       "cup_size           7.325288\n",
       "bra_size           6.994106\n",
       "height             1.211058\n",
       "quality            0.081392\n",
       "length             0.039293\n",
       "user_name          0.000000\n",
       "fit                0.000000\n",
       "user_id            0.000000\n",
       "category           0.000000\n",
       "size               0.000000\n",
       "item_id            0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data = pd.DataFrame({'num_missing': (df.isnull().sum()/df.shape[0])*100})\n",
    "missing_data.sort_values(\"num_missing\", ascending=False, inplace = True)\n",
    "missing_data\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysis th numb of missing value, the column that can be analyze by review text is shoe size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22958, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shoe_review = df[np.logical_and(pd.notnull(df[\"review_text\"]), pd.notnull(df[\"shoe_size\"]))][[\"user_name\",\"shoe_size\",\"review_text\"]]\n",
    "shoe_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003745970903388797\n",
      "585\n",
      "1201\n"
     ]
    }
   ],
   "source": [
    "count = np.sum([True if len(re.findall(r\"shoes|shoe\",x)) else False for x in shoe_review.review_text])\n",
    "\n",
    "num_true = count/shoe_review.shape[0]\n",
    "print(num_true)\n",
    "\n",
    "num_user_shoe = np.sum(shoe_review.groupby('user_name')['shoe_size'].unique().apply(lambda x: len(x))>1)\n",
    "print(num_user_shoe) ## differing sho_sizes not complete information\n",
    "\n",
    "num_user_all_df = np.sum(df.groupby('user_name')['shoe_size'].unique().apply(lambda x: len(x))>1)\n",
    "print(num_user_all_df) ##700 new data \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Handling the others column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review_summary.fillna(\"Unknown\", inplace=True)\n",
    "df.review_text.fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[['height','quality']] = imputer.fit_transform(df[['height','quality']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"] = df.length.fillna(df['length'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n"
     ]
    }
   ],
   "source": [
    "print(df.hips[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hips'].fillna('Unknown', inplace=True)\n",
    "df['hips'] = pd.to_numeric(df['hips'], errors='coerce')\n",
    "bins = [-2, 0, 31, 37, 40, 44, 75]\n",
    "labels = ['Unknown', 'XS', 'S', 'M', 'L', 'XL']\n",
    "df['hips'] = pd.cut(df['hips'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('review_summary', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_id = df.groupby(\"user_name\")['user_id'].apply(lambda x: len(np.unique(x))).reset_index()\n",
    "id_name = df.groupby(\"user_id\")['user_name'].apply(lambda x: len(np.unique(x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.user_name = df.user_name.apply(lambda x: x.lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/processed_data.csv')\n",
    "data = data.drop(\"Unnamed: 0\",axis =1)\n",
    "data  = data.drop(\"review_text\",axis=1)\n",
    "train, val = train_test_split(data, test_size=0.2)\n",
    "user_categorical_features = [\"user_name\",\"hips\",\"cup_size\"]\n",
    "user_numerical_features = [\"height\",\"bra_size\"]\n",
    "item_categorical_features = [\"item_id\", \"category\", \"length\"]\n",
    "item_numerical_features = [\"size\",\"quality\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "data_num = preprocessing.MinMaxScaler().fit(train[[\"height\",\"bra_size\",\"size\",\"quality\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"height\",\"bra_size\",\"size\",\"quality\"]] = data_num.transform(train[[\"height\",\"bra_size\",\"size\",\"quality\"]])\n",
    "val[[\"height\",\"bra_size\",\"size\",\"quality\"]] = data_num.transform(val[[\"height\",\"bra_size\",\"size\",\"quality\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in user_categorical_features + item_categorical_features:\n",
    "  df[col] = df[col].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_users = {\n",
    "    data_col : tf.feature_column.numeric_column(data_col) \\\n",
    "          for data_col in user_numerical_features\n",
    "}\n",
    "numeric_items = {\n",
    "    data_col : tf.feature_column.numeric_column(data_col) \\\n",
    "          for data_col in item_numerical_features\n",
    "}\n",
    "\n",
    "\n",
    "hips = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'hips', df.hips.unique().tolist())\n",
    "cup_size = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'cup_size', df.cup_size.unique().tolist())\n",
    "user_name = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'user_name', df.user_name.unique().tolist())\n",
    "\n",
    "\n",
    "\n",
    "item_id = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'item_id', df.item_id.unique().tolist())\n",
    "category = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'category',  df.category.unique().tolist())\n",
    "length = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      'length', df.length.unique().tolist())\n",
    "\n",
    "hips_embedding = tf.feature_column.embedding_column(hips, dimension=5)\n",
    "cup_size_embedding = tf.feature_column.embedding_column(cup_size, dimension=5)\n",
    "user_name_embedding = tf.feature_column.embedding_column(user_name, dimension=50)\n",
    "item_id_embedding = tf.feature_column.embedding_column(item_id, dimension=50)\n",
    "category_embedding = tf.feature_column.embedding_column(category, dimension=5)\n",
    "length_embedding = tf.feature_column.embedding_column(length, dimension=5)\n",
    "\n",
    "cat_users = {\n",
    "    'hips' : hips_embedding,\n",
    "    'cup_size' : cup_size_embedding,\n",
    "    'user_name': user_name_embedding\n",
    "}\n",
    "\n",
    "cat_items = {\n",
    "    'item_id' : item_id_embedding,\n",
    "    'category' : category_embedding,\n",
    "    'length': length_embedding\n",
    "}\n",
    "\n",
    "# input_user = {\n",
    "#     colname : tf.feature_column(\n",
    "#         tf.feature_column.numeric_column(colname, shape=(), dtype='float32')\n",
    "#     )\n",
    "#     for colname in numeric_users.keys()\n",
    "# }\n",
    "# input_user.update({\n",
    "#     colname : tf.feature_column(\n",
    "#         tf.feature_column.categorical_column_with_vocabulary_list(colname, shape=(),  dtype='string')\n",
    "#     )\n",
    "#           for colname in cat_users.keys()\n",
    "# })\n",
    "\n",
    "# input_items = {\n",
    "#     colname : tf.feature_column(\n",
    "#         tf.feature_column.numeric_column(colname, shape=(), dtype = 'float32')\n",
    "#     )\n",
    "#           for colname in numeric_items.keys()\n",
    "# }\n",
    "\n",
    "# input_items.update({\n",
    "#     colname : tf.feature_column(\n",
    "#         tf.feature_column.categorical_column_with_vocabulary_list(colname, shape=(),  dtype='string')\n",
    "#     )\n",
    "#           for colname in cat_items.keys()\n",
    "# })\n",
    "# input_user = {\n",
    "#     colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "#           for colname in numeric_users.keys()\n",
    "# }\n",
    "# input_user.update({\n",
    "#     colname_update : tf.keras.layers.Input(name=colname_update, shape=(),  dtype='string') \\\n",
    "#           for colname_update in cat_users.keys()\n",
    "# })\n",
    "\n",
    "# input_items = {\n",
    "#     colname_items : tf.keras.layers.Input(name=colname_items, shape=(), dtype = 'float32') \\\n",
    "#           for colname_items in numeric_items.keys()\n",
    "# }\n",
    "\n",
    "# input_items.update({\n",
    "#     colname_items : tf.keras.layers.Input(name=colname_items, shape=(),  dtype='string') \\\n",
    "#           for colname_items in cat_items.keys()\n",
    "# })\n",
    "\n",
    "input_user = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') \\\n",
    "          for colname in numeric_users.keys()\n",
    "}\n",
    "input_user.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in cat_users.keys()\n",
    "})\n",
    "\n",
    "input_items = {\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(), dtype = 'float32') \\\n",
    "          for colname in numeric_items.keys()\n",
    "}\n",
    "\n",
    "input_items.update({\n",
    "    colname : tf.keras.layers.Input(name=colname, shape=(),  dtype='string') \\\n",
    "          for colname in cat_items.keys()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature layer\n",
    "feature_layer_users = tf.keras.layers.DenseFeatures(numeric_users.values())(input_user)\n",
    "feature_layer_items = tf.keras.layers.DenseFeatures(numeric_items.values())(input_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS =  np.array([\"fit\",\"small\",\"large\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tensorflow import keras\n",
    "def convert_df(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('fit')\n",
    "  labels = labels.apply(lambda x:x == LABELS)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  #prefetching was giving some trouble on google colab,\n",
    "  #there might be some issue with some gdfs, hence not here\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipCon(keras.layers.Layer):\n",
    "  def __init__(self, size, reduce = True, deep = 3, skip_when=0, activation=\"relu\", **kwargs):\n",
    "    \"\"\"\n",
    "    @Params\n",
    "    size = size of dense layer\n",
    "    deep = the depth of network in one SkipCon block call\n",
    "    skip_when =  if a skip connection is required, pass 1\n",
    "    activation = by default using relu, in the paper authors have used tanh(no reasons again)\n",
    "    \"\"\"    \n",
    "    super().__init__(**kwargs)\n",
    "    self.activation = keras.activations.get(activation) # used to combine\n",
    "    # skip connections and cascaded dense layers\n",
    "    self.main_layers =[]\n",
    "    self.skip_when = skip_when #to be used in call as a control\n",
    "    if reduce:\n",
    "      for _ in range(deep):\n",
    "        self.main_layers.extend([\n",
    "          keras.layers.Dense(size, activation=activation, \n",
    "                              use_bias=True),\n",
    "          keras.layers.BatchNormalization()])\n",
    "\n",
    "        # Reduce the input size by two each time, if the\n",
    "        # network is to be designed deeper and narrow\n",
    "        size = size/2\n",
    "    else:\n",
    "      for _ in range(deep):\n",
    "        self.main_layers.extend([\n",
    "        keras.layers.Dense(size, activation=activation, \n",
    "                            use_bias=True),\n",
    "        keras.layers.BatchNormalization()])\n",
    "        \n",
    "    self.skip_layers = []\n",
    "    if skip_when > 0:\n",
    "      if reduce:\n",
    "        size = size*2 # since the size of skipped connection  \n",
    "                      # should match with cascaded dense\n",
    "      self.skip_layers = [\n",
    "          keras.layers.Dense(size, activation=activation, \n",
    "                          use_bias=True),\n",
    "          keras.layers.BatchNormalization()]\n",
    "\n",
    "  def call(self, inputs):\n",
    "    Z = inputs\n",
    "    for layer in self.main_layers:\n",
    "      Z = layer(Z)\n",
    "    if not self.skip_when:\n",
    "      return self.activation(Z)\n",
    "    skip_Z = inputs\n",
    "    for layer in self.skip_layers:\n",
    "      skip_Z = layer(skip_Z)\n",
    "    return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\util\\structure.py:104\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    105\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\util\\structure.py:507\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    504\u001b[0m     logging\u001b[39m.\u001b[39mvlog(\n\u001b[0;32m    505\u001b[0m         \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to convert \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m to tensor: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, e))\n\u001b[1;32m--> 507\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCould not build a `TypeSpec` for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    508\u001b[0m     element,\n\u001b[0;32m    509\u001b[0m     \u001b[39mtype\u001b[39m(element)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a `TypeSpec` for 45707    [True, False, False]\n60828    [True, False, False]\n16187    [True, False, False]\n24530    [True, False, False]\n27671    [False, True, False]\n                 ...         \n1260     [True, False, False]\n33283    [True, False, False]\n18034    [True, False, False]\n34400    [True, False, False]\n55725    [True, False, False]\nName: fit, Length: 57008, dtype: object with type Series",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\body-measurement-using-cv\\train_model.ipynb Cell 46\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# train.drop(['bra_size','cup_size'], axis=1)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# val.drop(['bra_size','cup_size'], axis=1)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_ds \u001b[39m=\u001b[39m convert_df(train, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m val_ds \u001b[39m=\u001b[39m convert_df(val, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "\u001b[1;32me:\\body-measurement-using-cv\\train_model.ipynb Cell 46\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m labels \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:x \u001b[39m==\u001b[39m LABELS)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_tensor_slices((\u001b[39mdict\u001b[39;49m(dataframe), labels))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m shuffle:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/body-measurement-using-cv/train_model.ipynb#Y106sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mshuffle(buffer_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataframe))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:830\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[39m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m    829\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_tensor_slices_op\n\u001b[1;32m--> 830\u001b[0m \u001b[39mreturn\u001b[39;00m from_tensor_slices_op\u001b[39m.\u001b[39;49m_from_tensor_slices(tensors, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 25\u001b[0m   \u001b[39mreturn\u001b[39;00m _TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, element, is_files\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m   element \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39;49mnormalize_element(element)\n\u001b[0;32m     34\u001b[0m   batched_spec \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m     35\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\util\\structure.py:109\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    104\u001b[0m     spec \u001b[39m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m   \u001b[39m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[0;32m    107\u001b[0m   \u001b[39m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m   normalized_components\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 109\u001b[0m       ops\u001b[39m.\u001b[39;49mconvert_to_tensor(t, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcomponent_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m i))\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m   \u001b[39m# To avoid a circular dependency between dataset_ops and structure,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m   \u001b[39m# we check the class name instead of using `isinstance`.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m   \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDatasetSpec\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1633\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1635\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1638\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1639\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[0;32m   1641\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1642\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1644\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1645\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:344\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    342\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    343\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> 344\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    173\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \n\u001b[0;32m    175\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    269\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    279\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 280\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    282\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m    283\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[0;32m    306\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "batch_size = 512 \n",
    "# train.drop(['bra_size','cup_size'], axis=1)\n",
    "# val.drop(['bra_size','cup_size'], axis=1)\n",
    "train_ds = convert_df(train, batch_size=batch_size)\n",
    "val_ds = convert_df(val, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
